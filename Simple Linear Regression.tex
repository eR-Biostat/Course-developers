%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% THIS TEX FILE WAS USED TO PRODUCE THE SLIDES FOR THE CLASS: SIMPLE LINEAR REGRESSION %%
%% Written by Legesse Kassa Debusho and Ziv Shkedy                                      %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\documentclass{beamer}
\setbeamertemplate{navigation symbols}{}
\usepackage{rotating}
\usepackage{latexsym}
\usepackage{mathrsfs}
\usepackage{amssymb,amsfonts}
\usepackage{amssymb,mathrsfs}
\usepackage{amscd}
\usepackage {graphicx}
\usepackage{epsfig}
\usepackage{amsmath, amssymb}
\usepackage{verbatim}
\usepackage{color}
%\input{rgb}


\usetheme{Warsaw}

\beamersetuncovermixins{\opaqueness<1>{25}}{\opaqueness<2->{15}}
\begin{document}
\def \bb{\mbox{\boldmath $\beta$}}
\def \mm{\mbox{\boldmath $\mu$}}
\def \ee{\mbox{\boldmath $\varepsilon$}}
\def \vv{\mbox{\boldmath $V$}}
\def \xx{\mbox{\boldmath $X$}}
\def \tt{\mbox{\boldmath $\theta$}}
\def \ll{\mbox{\boldmath $\lambda$}}
\def \s{\sigma}
\def \b{\beta}
\def \g{\gamma}
\def \ds{\displaystyle}
\def \ul{\underline}
\def \R{I \!\! R}
\def \d{\delta}
\def \r{\rho}
\def \tilt{\tilde{t}}
\def \ss{\mbox{\boldmath $\Sigma$}}
\def \bt{\mbox{\boldmath $\tau$}}
\def \aa{\mbox{\boldmath $\alpha$}}
\def \ee{\mbox{\boldmath $\epsilon$}}
\def \ll{\mbox{\boldmath $\lambda$}}
\def \t{\mbox{\boldmath $\tau$}}

\title{Simple Linear Regression}
\author{Legesse Kassa Debusho, UNISA, South Africa and Ziv Shkedy, Hasselt University, Belgium}
\date{\today}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}\frametitle{Table of contents}\tableofcontents
\end{frame}

\section{Introduction}
\begin{frame}\frametitle{Introduction}
\begin{itemize}
\item Regression analysis is a body of knowledge dealing with the formulation of mathematical models that depict relationships among variables.
\item It is used for explaining or modeling the relationship between a single variable $Y$, called the response, output or dependent variable, and one or more predictor, input, independent or explanatory variables, $X_1, X_2,\ldots, X_p$.
\item When $p=1$, it is called simple regression but when $p \ge 1$ it is called multiple regression (this will be discussed in Chapter 3).
\item In simple regression a single predictor or independent variable is used for predicting the variable of interest, i.e. dependent variable.
\item In this course we will consider parametric regression models and we will assume a relationship that is linear in the parameters.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Introduction}
\begin{itemize}
\item Therefore, a simple linear regression (SLR) attempts to model the relationship between a single explanatory or predictor variable and a response variable by fitting an equation to observed data that is linear in the parameters.
\item The parameters are called regression parameters or regression coefficients.
\item Both the response and independent variables in SLR must be a continuous variables.
\item Objectives of regression analyses can be
   \begin{itemize}
       \item to predict a continuous dependent variable from one or a number of independent variables.
       \item to check whether there is a relationship between a dependent or response variable and one or more than one independent variables.
       \item to describe the data structure.
   \end{itemize}
\end{itemize}
\end{frame}


\section{The Simple linear regression model}
\begin{frame}\frametitle{The simple linear regression model}
\begin{itemize}
\item The model expresses the value of a response variable as a linear function of a predictor variable and an error term. The simple linear regression model can be stated as follows:
\begin{equation}
y_i = \beta_0 + \beta_1\,x_i + \varepsilon_i,  \;\;\; i=1,...,n
\end{equation}
where
\begin{itemize}
\item $y_i$ represents the $i$th value of the response variable $Y$,
\item $x_i$ represents the $i$th value of the predictor variable $X$,
\item $\beta_0$ and $\beta_1$ are constants called regression coefficients or parameters.
\item $\varepsilon_i$ is a random disturbance or error.
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}\frametitle{The simple linear regression model}
\begin{itemize}
\item The coefficient $\beta_1$, called the slope, may be interpreted as the change in $Y$ for a unit change in $X$.
\item The coefficient $\beta_0$, called the constant coefficient or intercept, is the predicted value of $Y$ when $X=0$.
\item Note that the random term $\varepsilon_i$ does not contain any systematic information for determining $y_i$ that is not already captured by $x_i$.
\end{itemize}
\end{frame}


\begin{frame}\frametitle{Matrix notation of simple linear regression model}
\begin{itemize}
\item Model (1) implies that
\begin{eqnarray}
\left\{ \begin{array}{ccl}
y_1& =& \beta_0 + \beta_1 x_1 +  \varepsilon_1\\
\vdots \\
y_i& =& \beta_0 + \beta_1 x_i + \varepsilon_i\\
\vdots \\
y_n& =& \beta_0 + \beta_1 x_n + \varepsilon_n
\end{array} \right.
\end{eqnarray}
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Matrix notation of simple linear regression model}
\begin{itemize}
\item This can be written in matrix form as follows
\begin{eqnarray*}
\left( \begin{array}{c} y_1 \\ y_2\\ \vdots\\  y_n \end{array} \right) = \left(
\begin{array}{cc} 1 & x_1 \\ 1 & x_2 \\ \vdots\\ 1 & x_n \end{array} \right) \left( \begin{array}{c} \beta_0 \\
\beta_1 \end{array} \right) + \left( \begin{array}{c} \varepsilon_1 \\
\varepsilon_2 \\ \vdots\\ \varepsilon_n \end{array} \right)
\end{eqnarray*}
\item or
\begin{eqnarray}
\begin{array}{cccccc}
{\bf y} &=&{\bf X}&\bb & +&\ee \\
(n\times 1) & &(n \times 2) &(2 \times 1) & &(n \times 1)
\end{array}
\end{eqnarray}
\end{itemize}
\end{frame}

\subsection{Assumptions for regression analysis}
\begin{frame}\frametitle{Assumptions}
\begin{itemize}
\item The errors have an expected value of zero, i.e. $E(\varepsilon_i)$ or $E(\ee) = {\bf 0}$. This means that on average the errors balance out.
\item The independent variable is non-random. In an experiment, the values of the independent variable would be fixed by the experimenter and repeated samples could be drawn with the independent variable fixed at the same values in each sample.
\item  The disturbances are homoscedastic. This means that the variance of the disturbance is the same for each observation, i.e. $Var(\varepsilon_i) = \s^2$, for all $i$ or $Var(\ee) = \s^2\,{\bf I}_n$.
\item The disturbances are not autocorrelated. This means disturbances associated with different observations are uncorrelated.
\item For statistical inferences (confidence intervals and test of hypothesis), the distribution of $\varepsilon_i$ is normal.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Assumptions}
\begin{itemize}
\item In short, the above assumptions related to $\varepsilon_i$ can be stated as $\varepsilon_i \sim \mathcal N(0, \s^2)$ or $\ee \sim \mathcal N({\bf 0}, s^2\,{\bf I}_n)$.
\item Note the following
   \begin{itemize}
      \item From model (1), note that the observed value of $Y$ in the $i$th observation is the sum of two components, namely the constant term $\b_0 + \b_1\,x_i$ and the random error term $\varepsilon_i$. Hence $y_i$ is a random variable.
      \item From the above assumptions it follow that
        \begin{itemize}
        \item[(i)] $E(y_i) = E(\b_0 + \b_1\,x_i + \varepsilon_i) = \b_0 + \b_1\,x_i$.
        \item[(ii)] $Var(y_i) = Var(\b_0 + \b_1\,x_i + \varepsilon_i)=Var(\varepsilon_i) = \s^2$.
        \item[(iii)] $Cov(Y_i,Y_j) = 0$, i.e., the response $y_i$ and $y_j$ are uncorrelated.
        \item[(iv)] Using matrix notation, ${\bf y} \sim \mathcal N({\bf X}\,\bb, \s^2\,{\bf I}_n)$.
        \end{itemize}
    \end{itemize}
\end{itemize}
\end{frame}

\subsubsection{Least squares estimation}
\begin{frame}\frametitle{Least squares estimation}
\begin{itemize}
\item The least squares criterion states that the estimator $\widehat{\bb}$ of $\bb$ must be found in such a way that $\ee'\ee$, the sum of squares of the residuals, is a minimum. That is, to find the least squares estimates we have to minimise
\begin{eqnarray*}
Q(\bb)  & = & \ee'\ee = \sum_{i=1}^n\varepsilon_i^2 = ({\bf y- X} \bb)'{\bf (y-X}\bb) \\
        & = & {\bf y'y} -\bb'{\bf X' y} -{\bf y' X}\bb+\bb'{\bf X' X}\bb
\end{eqnarray*}
\item  In order to minimise $Q(\bb)$ we set $\left.\ds\frac{d Q(\bb)}{d\bb}\right|_{\bb=\widehat{\bb}} ={\bf 0}$. We find that
$$
 {\bf X'X} \widehat{\bb} = {\bf X'y}
$$
and these are called the normal equations.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Least squares estimation}
\begin{itemize}
\item Provided that ${\bf X}$ has full column rank or ${\bf X}'{\bf X}$ is non-singular, the least square estimates are given by
$$
\widehat{\bb} ={ \bf (X'X)^{-1} X'y}.
$$
\item Hence the least squares regression line is given by $\widehat{Y} = \widehat{\b}_0 + \widehat{\b}_1\,X$.
\item For simple linear regression the least squares estimates can also be given by
$$
\widehat{\b}_1 = \frac{\sum_{i=1}^n(y_i - \bar{y})(x_i - \bar{x})}{\sum_{i=1}^n(x_i - \bar{x})^2}
$$
and
$$
\widehat{\b}_0 = \bar{y} - \widehat{\b}_1\,\bar{x}.
$$
\end{itemize}
\end{frame}


\subsection{Fitting simple linear regression model using R}
\begin{frame}\frametitle{Example: Galapagos Islands Data}
\begin{itemize}
\item A data frame with 30 Galapagos islands and 7 variables is available in R under the library named faraway:
  \begin{itemize}
     \item Species:  the number of plant species found on the island
     \item Endemics:  the number of endemic species
     \item Area:  the area of the island (km$^2$)
     \item Elevation: the highest elevation of the island (m)
     \item Nearest:  the distance from the nearest island (km)
     \item Scruz:  the distance from Santa Cruz island (km)
     \item Adjacent:  the area of the adjacent island (square km)
  \end{itemize}
\item The data were presented by Johnson and Raven (1973) and also appear in Weisberg (1985). The original dataset contained several missing values which have been filled for convenience.
\end{itemize}
\end{frame}

\begin{frame}[fragile]\frametitle{Example: Galapagos Islands Data}
\begin{itemize}
\item The relationship between the number of plant species and several geographic variables is of interest.
\item In this chapter we are interested in the relationship between the number of plant species and the area of the adjacent island (square km).
\item As a preliminary analysis a scatter plot of the two variables are presented in Figure 2.1 below.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Example: Galapagos Islands Data}
\begin{figure}[!hbp]
\centering
\includegraphics[height=4cm]{Figure2_1.eps}
\end{figure}
\begin{itemize}
\item Figure 2.1 shows that the relationship between the number of species found on the island and the highest elevation of the island (m) can be described by a linear model.
\end{itemize}
\end{frame}

\begin{frame}[fragile]\frametitle{Example: Galapagos Islands Data}
\begin{itemize}
\item Fitting a simple linear regression model in R is done using the {\color{red} lm()} command. The general arguments of the command shown below but the main argument is "formula".
\end{itemize}
\begin{verbatim}
lm(formula, data, subset, weights, na.action,
   method = "qr", model = TRUE, x = FALSE, y = FALSE,
   qr = TRUE, singular.ok = TRUE, contrasts = NULL,
\end{verbatim}
\end{frame}

\begin{frame}[fragile]\frametitle{Example: Galapagos Islands Data}
\begin{itemize}
\item First read the data into R as shown below.
\begin{verbatim}
library(faraway)
data(gala)
attach(gala)
\end{verbatim}
\item The {\color{blue} library(faraway)} makes the data used in this book available while {\color{blue} data(gala)} calls up this particular dataset.
\item Then the model fitted using the {\color{red} lm()} command as
\begin{verbatim}
SLR.fit.1 <- lm(Species~Elevation)
summary(SLR.fit.1)
\end{verbatim}
\item The result of fitted model is stored in an object called "SLR.fit.1". The order of the variables is \emph{dependent} followed by a tilde "$\sim$" followed by \emph{a list of independent variables}.
\item The command {\color{blue} summary} creates a summary of fitted model.
\end{itemize}
\end{frame}

\begin{frame}[fragile]\frametitle{Example: Galapagos Islands Data}
\begin{verbatim}
Call:
lm(formula = Species ~ Elevation)
Residuals:
     Min       1Q   Median       3Q      Max
-218.319  -30.721  -14.690    4.634  259.180
Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept) 11.33511   19.20529   0.590     0.56
Elevation    0.20079    0.03465   5.795 3.18e-06 ***
---
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 78.66 on 28 degrees of freedom
Multiple R-squared:  0.5454,  Adjusted R-squared:  0.5291
F-statistic: 33.59 on 1 and 28 DF,  p-value: 3.177e-06
\end{verbatim}
\end{frame}

\begin{frame}[fragile]\frametitle{Example: Galapagos Islands Data}
\begin{itemize}
\item The above output contains a lot of information but for now we will focus on the fitted model.
\item The estimates for the model intercept and the slope are 11.33511 and 0.20079. Therefore, the fitted or the least squares regression line is
    $$
    \hat{y} = 11.335 + 0.201\,x
    $$
\item The slope show that there is a positive relationship (consistent with the scatter plot), in other words, as the elevation of the island increase there will be an increase in the number of species of tortoise found on the island.
\item The multiple R-squared or $R^2 = 0.5454$ (is also known as the coefficient of determination) may be interpreted as the proportion of the total variability in the number of species of tortoise found on the island (i.e. the response variable) that is accounted for the elevation of the island (i.e. the predictor variable). Therefore, 54.54\% of the total variability in the number of species of tortoise found on the island is accounted for by the elevation of the island.
\item For simple linear regression, the correlation coefficient can be calculated using the multiple R-squared value from the above output as $r = \sqrt{0.5454} \approx 0.74$. This shows that there is a strong positive linear relationship between the two variables.
\end{itemize}
\end{frame}


\section{Properties of the least square estimator of  $\bb$}
\begin{frame}\frametitle{Properties of the least square estimator of  $\bb$}
\begin{itemize}
\item Recall that for the general linear model ${\bf y}= \xx\bb + \ee$ we assume that
\item $E (\bf \varepsilon) = {\bf 0}$, ~~ $Var(\ee) = \s^2{\bf I_n}$ or $\ee \sim \mathcal({\bf 0}, \s^2{\bf I}_n)$.
\item Furthermore, do to these assumptions we have ${\bf y} \sim \mathcal({\bf X} \bb, \s^2{\bf I}_n)$.
\item Thus, for $\widehat{\bb} ={ \bf (X'X)^{-1} X'y}$
\begin{description}
\item (i) $E(\widehat{\bb})=E({\bf (X'X)^{-1} X'y})=(\xx'\xx)^{-1} \xx'E({\bf y})=({\bf X}'{\bf X})^{-1} {\bf X}'{\bf X}\bb = {\bb}$ and
\item (ii) $Var(\widehat{\bb})=Var{\bf ((X'X)^{-1} X'y)}= \s^2{\bf (X'X)^{-1} X' I_n X (X'X)^{-1}} = \s^2{\bf (X'X)^{-1}}$.
\end{description}
\end{itemize}
\end{frame}

\section{Tests of hypotheses on the regression coefficients}
\begin{frame}\frametitle{Properties of the least square estimator of  $\bb$}
\begin{itemize}
\item For simple linear regression
$$
({\bf X}'{\bf X})^{-1} = \left ( \begin{array}{cc} \frac{\sum_{i=1}^n x_i^2}{n\,\sum_{i=1}^n(x_i - \bar{x})^2} & - \frac{\sum_{i=1}^n x_i}{n\,\sum_{i=1}^n (x_i - \bar{x})^2} \\ - \frac{\sum_{i=1}^n x_i}{n\,\sum_{i=1}^n (x_i - \bar{x})^2} & \frac{1}{n\,\sum_{i=1}^n (x_i - \bar{x})^2} \end{array} \right ).
$$
\item Thus $Var(\widehat{\bb}) = \s^2({\bf X}'{\bf X})^{-1}$ is a variance-covariance matrix with
\begin{description}
\item $Var(\widehat{\b_0})= \s^2\,\sum_{i=1}^n x_i^2/n\,\sum_{i=1}^n(x_i - \bar{x})^2= \s^2 \left [\frac{1}{n} + \frac{\bar{x}}{\sum_{i=1}^n(x_i - \bar{x})^2} \right ]$,
\item $Var(\widehat{\b_1})= \s^2\,/\sum_{i=1}^n (x_i - \bar{x})^2$, and
\item $Cov(\widehat{\b_0}, \widehat{\b_1})= -\s^2\,\sum_{i=1}^n x_i / n\,\sum_{i=1}^n (x_i - \bar{x})^2$.
\end{description}
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Estimating $\s^2$}
\begin{itemize}
\item The variances of $\widehat{\b_0}$ and $\widehat{\b_1}$ depend on the unknown parameter $\s^2$. So, 

\end{itemize}
\end{frame}

\end{document} 